{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuwAfpo5rEKy",
    "outputId": "bd0b5636-cc23-4ba9-8442-6aa0f6e44af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 228s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 0us/step\n",
      "castle (34.03%)\n"
     ]
    }
   ],
   "source": [
    "# example of using a pre-trained model as a classifier\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load an image from file\n",
    "image = load_img('download.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valley (44.85%)\n"
     ]
    }
   ],
   "source": [
    "# load an image from file\n",
    "image = load_img('download2.png', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded MNIST dataset locally!\n",
      "Training data shape: (10000, 28, 28)\n",
      "Test data shape: (2000, 28, 28)\n",
      "✅ Preprocessing complete.\n",
      "x_train: (10000, 32, 32, 3) y_train: (10000, 10)\n",
      "x_test : (2000, 32, 32, 3) y_test : (2000, 10)\n",
      "Epoch 1/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 687ms/step - accuracy: 0.7569 - loss: 0.8457 - val_accuracy: 0.8465 - val_loss: 0.5126\n",
      "Epoch 2/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 699ms/step - accuracy: 0.8968 - loss: 0.3603 - val_accuracy: 0.8935 - val_loss: 0.3445\n",
      "Epoch 3/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 698ms/step - accuracy: 0.9237 - loss: 0.2639 - val_accuracy: 0.9165 - val_loss: 0.2786\n",
      "Epoch 4/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 716ms/step - accuracy: 0.9354 - loss: 0.2161 - val_accuracy: 0.9305 - val_loss: 0.2376\n",
      "Epoch 5/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 686ms/step - accuracy: 0.9430 - loss: 0.1900 - val_accuracy: 0.9345 - val_loss: 0.2145\n",
      "✅ Test Accuracy: 0.9345\n",
      "Test Loss: 0.2145\n"
     ]
    }
   ],
   "source": [
    "# ANOTHER CODE \n",
    "# --- Stage 1: Load MNIST dataset locally ---\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Load local dataset\n",
    "data_path = r\"mnist_dataset.npz\"\n",
    "\n",
    "with np.load(data_path) as data:\n",
    "    X_train = data['X_train'][:10000]\n",
    "    y_train = data['y_train'][:10000]\n",
    "    X_test = data['X_test'][:2000]\n",
    "    y_test = data['y_test'][:2000]\n",
    "\n",
    "print(\"✅ Loaded MNIST dataset locally!\")\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "# --- Stage 2: Preprocess data ---\n",
    "# MNIST images are 28x28 grayscale — add channel dim\n",
    "x_train = np.expand_dims(X_train, -1)  # (28,28,1)\n",
    "x_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "# Resize to 32x32 for VGG16 and convert to RGB (3 channels)\n",
    "x_train = tf.image.resize(x_train, [32, 32])\n",
    "x_test = tf.image.resize(x_test, [32, 32])\n",
    "x_train = tf.image.grayscale_to_rgb(x_train)\n",
    "x_test = tf.image.grayscale_to_rgb(x_test)\n",
    "\n",
    "# Normalize pixel values to [0,1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"✅ Preprocessing complete.\")\n",
    "print(\"x_train:\", x_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"x_test :\", x_test.shape, \"y_test :\", y_test.shape)\n",
    "\n",
    "# --- Stage 3: Load pre-trained VGG16 (without top layers) ---\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Freeze convolutional base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# --- Stage 4: Build the model ---\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # MNIST has 10 classes\n",
    "])\n",
    "\n",
    "# --- Stage 5: Compile the model ---\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- Stage 6: Train the model ---\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Stage 7: Evaluate on test set ---\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_train', 'y_train', 'X_test', 'y_test']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Load local dataset\n",
    "data_path = r\"mnist_dataset.npz\"\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
